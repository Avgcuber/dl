{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Uhn7qELsmxjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionSGD(LogisticRegression):\n",
        "    def gradient_descent(self, X, y, learning_rate, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(len(X)):\n",
        "                prediction = self.predict(X[i])\n",
        "                error = y[i] - prediction\n",
        "                self.weights += learning_rate * error * X[i]\n",
        "                self.bias += learning_rate * error\n",
        "\n",
        "learning_rate_sgd = 0.01\n",
        "epochs_sgd = 100\n",
        "model_sgd = LogisticRegressionSGD(X_train.shape[1])\n",
        "model_sgd.gradient_descent(X_train, y_train, learning_rate_sgd, epochs_sgd)\n",
        "\n",
        "y_pred_sgd = model_sgd.predict(X_test)\n",
        "accuracy_sgd = accuracy_score(y_test, y_pred_sgd)\n",
        "print(f\"Stochastic GD Accuracy: {accuracy_sgd}\")\n"
      ],
      "metadata": {
        "id": "DmO30gvrnSZd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d76b8f9-abc3-4996-d448-34e657f83b2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stochastic GD Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionMiniBatch(LogisticRegression):\n",
        "    def gradient_descent(self, X, y, learning_rate, epochs, batch_size):\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(0, len(X), batch_size):\n",
        "                batch_X = X[i:i+batch_size]\n",
        "                batch_y = y[i:i+batch_size]\n",
        "                prediction = np.round(self.sigmoid(np.dot(batch_X, self.weights) + self.bias))\n",
        "                error = batch_y - prediction\n",
        "                self.weights += learning_rate * np.dot(batch_X.T, error) / batch_size\n",
        "                self.bias += learning_rate * np.sum(error) / batch_size\n",
        "\n",
        "learning_rate_mini_batch = 0.01\n",
        "epochs_mini_batch = 100\n",
        "batch_size_mini_batch = 32\n",
        "model_mini_batch = LogisticRegressionMiniBatch(X_train.shape[1])\n",
        "model_mini_batch.gradient_descent(X_train, y_train, learning_rate_mini_batch, epochs_mini_batch, batch_size_mini_batch)\n",
        "\n",
        "y_pred_mini_batch = model_mini_batch.predict(X_test)\n",
        "accuracy_mini_batch = accuracy_score(y_test, y_pred_mini_batch)\n",
        "print(f\"Mini Batch Accuracy: {accuracy_mini_batch}\")\n"
      ],
      "metadata": {
        "id": "DLD47spWnzVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "857c81eb-02c9-4826-bcbc-99df46a652e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mini Batch Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionAdagrad(LogisticRegression):\n",
        "    def gradient_descent(self, X, y, learning_rate, epochs):\n",
        "        grad_squared_sum = np.zeros(X.shape[1])\n",
        "        bias_grad_squared_sum = 0\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(len(X)):\n",
        "                prediction = np.round(self.sigmoid(np.dot(X[i], self.weights) + self.bias))\n",
        "                error = y[i] - prediction\n",
        "                d_weights = -error * X[i]\n",
        "                d_bias = -error\n",
        "\n",
        "                grad_squared_sum += d_weights ** 2\n",
        "                bias_grad_squared_sum += d_bias ** 2\n",
        "\n",
        "                self.weights -= (learning_rate / np.sqrt(grad_squared_sum + 1e-8)) * d_weights\n",
        "                self.bias -= (learning_rate / np.sqrt(bias_grad_squared_sum + 1e-8)) * d_bias\n",
        "\n",
        "learning_rate_adagrad = 0.01\n",
        "epochs_adagrad = 100\n",
        "model_adagrad = LogisticRegressionAdagrad(X_train.shape[1])\n",
        "model_adagrad.gradient_descent(X_train, y_train, learning_rate_adagrad, epochs_adagrad)\n",
        "\n",
        "y_pred_adagrad = model_adagrad.predict(X_test)\n",
        "accuracy_adagrad = accuracy_score(y_test, y_pred_adagrad)\n",
        "print(f\"Adagrad Accuracy: {accuracy_adagrad}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXXvu-SOom1B",
        "outputId": "9cbb17ee-dfb3-4fc1-f0ec-ef61e7cfd999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adagrad Accuracy: 0.9473684210526315\n"
          ]
        }
      ]
    }
  ]
}